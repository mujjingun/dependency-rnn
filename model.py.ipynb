{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import jamo\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = 'ᐁ' # Root of sentence symbol\n",
    "MS = 'ᑌ' # morpheme separator symbol\n",
    "WS = 'ᐯ' # word separator symbol\n",
    "EOS = 'ᕒ' # end of sentence symbol\n",
    "ESC_BEGIN = 'ᐸ' # beginning of escape sequence symbol\n",
    "ESC_END = 'ᐳ' # end of escape sequence symbol\n",
    "PADDING = 'ᒣ' # padding after end-of-sentence\n",
    "MASK = 'ᗰ' # masking symbol\n",
    "hangul = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'] + \\\n",
    "    [ESC_BEGIN, ESC_END, ROOT, MS, WS, EOS, PADDING, MASK] + \\\n",
    "    [chr(i) for i in range(0x1100, 0x1113)] + \\\n",
    "    [chr(i) for i in range(0x1161, 0x1176)] + \\\n",
    "    [chr(i) for i in range(0x11A8, 0x11C3)] + \\\n",
    "    [' ', '(', ')', '.', ',', '?', '\\\"\"', '\\'']\n",
    "PADDING_IDX = hangul.index(PADDING)\n",
    "MASK_IDX = hangul.index(MASK)\n",
    "def encode_string(s):\n",
    "    compat_jamos = [chr(i) for i in range(0x3131, 0x314f)]\n",
    "    s = jamo.h2j(s)\n",
    "    s = \"\".join(jamo.hcj2j(ch, \"tail\") if ch in compat_jamos else ch for ch in s)\n",
    "    def escape(ch):\n",
    "        return [10] + [hangul.index(c) for c in str(ord(ch))] + [11]\n",
    "    result = []\n",
    "    for ch in s:\n",
    "        if ch in hangul:\n",
    "            result.append(hangul.index(ch))\n",
    "        else:\n",
    "            result += escape(ch)\n",
    "    return result\n",
    "\n",
    "def decode_string(s):\n",
    "    result = []\n",
    "    i = 0\n",
    "    while i < len(s):\n",
    "        if s[i] == 10:\n",
    "            ch = ''\n",
    "            i += 1\n",
    "            while s[i] != 11:\n",
    "                ch += str(s[i])\n",
    "                i += 1\n",
    "            result.append(chr(int(ch)))\n",
    "        else:\n",
    "            result.append(hangul[s[i]])\n",
    "        i += 1\n",
    "    return \"\".join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_in_length = 600\n",
    "max_out_length = 700\n",
    "max_dep_length = 90\n",
    "\n",
    "\"\"\"\n",
    "CoNLL-U Format\n",
    "ID: Word index, integer starting at 1 for each new sentence; may be a range for multiword tokens; may be a decimal number for empty nodes (decimal numbers can be lower than 1 but must be greater than 0).\n",
    "FORM: Word form or punctuation symbol.\n",
    "LEMMA: Lemma or stem of word form.\n",
    "UPOS: Universal part-of-speech tag.\n",
    "XPOS: Language-specific part-of-speech tag; underscore if not available.\n",
    "FEATS: List of morphological features from the universal feature inventory or from a defined language-specific extension; underscore if not available.\n",
    "HEAD: Head of the current word, which is either a value of ID or zero (0).\n",
    "DEPREL: Universal dependency relation to the HEAD (root iff HEAD = 0) or a defined language-specific subtype of one.\n",
    "DEPS: Enhanced dependency graph in the form of a list of head-deprel pairs.\n",
    "MISC: Any other annotation.\n",
    "\"\"\"\n",
    "def read_conllu(filenames):\n",
    "    texts = []\n",
    "    morphs = []\n",
    "    depends = []\n",
    "    for filename in filenames:\n",
    "        with open(filename) as fp:\n",
    "            for line in fp.readlines():\n",
    "                if line.startswith('#'):\n",
    "                    if line.startswith('# text = '):\n",
    "                        texts.append([])\n",
    "                        morphs.append([ROOT])\n",
    "                        depends.append([])\n",
    "                else:\n",
    "                    split = line.split('\\t')\n",
    "                    if len(split) != 10:\n",
    "                        continue\n",
    "                    idx, form, lemma, upos, xpos, feats, head, deprel, deps, misc = split\n",
    "                    \n",
    "                    if \"SpaceAfter=No\" not in misc:\n",
    "                        form += ' '\n",
    "                    texts[-1].append(encode_string(form))\n",
    "                    \n",
    "                    lemma = MS.join(lemma.split('+')) + WS\n",
    "                    morphs[-1].append(lemma)\n",
    "                    depends[-1].append(int(head) - 1)\n",
    "    \n",
    "    for i in range(len(morphs)):\n",
    "        morphs[i][-1] += EOS\n",
    "    morphs = [[encode_string(w) for w in m] for m in morphs]\n",
    "    \n",
    "    dep_lengths = []\n",
    "    depend_idxs = []\n",
    "    depend_aligns = []\n",
    "    for text, morph, depend in zip(texts, morphs, depends):\n",
    "        word_cum_lengths = np.cumsum([len(w) for w in text])\n",
    "        morph_cum_lengths = np.cumsum([len(w) for w in morph])\n",
    "        \n",
    "        indices = morph_cum_lengths[1:] - 1\n",
    "        pad_size = max_dep_length - len(indices)\n",
    "        indices = np.pad(indices, (0, pad_size), 'constant')\n",
    "        depend_idxs.append(indices)\n",
    "        \n",
    "        for i in range(len(depend)):\n",
    "            depend[i] = word_cum_lengths[i] - 1\n",
    "        dep_lengths.append(len(depend))\n",
    "        depend_aligns.append(np.pad(depend, (0, pad_size), 'constant'))\n",
    "    \n",
    "    text_lengths = []\n",
    "    for i in range(len(texts)):\n",
    "        texts[i] = sum(texts[i], [])\n",
    "        text_lengths.append(len(texts[i]))\n",
    "        texts[i] += [PADDING_IDX] * (max_in_length - len(texts[i]))\n",
    "        \n",
    "    out_lengths = []\n",
    "    for i in range(len(morphs)):\n",
    "        morphs[i] = sum(morphs[i], [])\n",
    "        out_lengths.append(len(morphs[i]))\n",
    "        morphs[i] += [PADDING_IDX] * (max_out_length - len(morphs[i]))\n",
    "    \n",
    "    return {'inputs': np.array(texts, dtype=np.int32), \n",
    "            'in_lengths': np.array(text_lengths, dtype=np.int32),\n",
    "            'depend_idxs': np.array(depend_idxs, dtype=np.int32),\n",
    "            'depends': np.array(depend_aligns, dtype=np.int32),\n",
    "            'dep_lengths': np.array(dep_lengths, dtype=np.int32), \n",
    "            'morphs': np.array(morphs, dtype=np.int32),\n",
    "            'out_lengths': np.array(out_lengths, dtype=np.int32)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading input files...\n",
      "Reading from .npy...\n",
      "Done.\n",
      "Training set size: 27410\n",
      "Test set size: 3276\n"
     ]
    }
   ],
   "source": [
    "reparse = False\n",
    "\n",
    "print(\"Reading input files...\")\n",
    "if not reparse and os.path.exists('train.npy'):\n",
    "    print(\"Reading from .npy...\")\n",
    "    train = np.load('train.npy').item()\n",
    "    test = np.load('test.npy').item()\n",
    "else:\n",
    "    print(\"Parsing ConLLU database...\")\n",
    "    train = read_conllu([\n",
    "        'UD_Korean-GSD/ko_gsd-ud-train.conllu',\n",
    "        'UD_Korean-Kaist/ko_kaist-ud-train.conllu'])\n",
    "    test  = read_conllu([\n",
    "        'UD_Korean-GSD/ko_gsd-ud-test.conllu',\n",
    "        'UD_Korean-Kaist/ko_kaist-ud-test.conllu'])\n",
    "\n",
    "    # Save to file for later\n",
    "    np.save('train.npy', train)\n",
    "    np.save('test.npy', test)\n",
    "    \n",
    "print(\"Done.\")\n",
    "print(\"Training set size:\", len(train['inputs']))\n",
    "print(\"Test set size:\", len(test['inputs']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcatOutputAndAttentionWrapper(tf.contrib.rnn.RNNCell):\n",
    "    '''Concatenates RNN cell output with the attention context vector.\n",
    "\n",
    "    This is expected to wrap a cell wrapped with an AttentionWrapper constructed with\n",
    "    attention_layer_size=None and output_attention=False. Such a cell's state will include an\n",
    "    \"attention\" field that is the context vector.\n",
    "    '''\n",
    "    def __init__(self, cell):\n",
    "        super(ConcatOutputAndAttentionWrapper, self).__init__()\n",
    "        self._cell = cell\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return self._cell.state_size\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self._cell.output_size + self._cell.state_size.attention\n",
    "\n",
    "    def call(self, inputs, state):\n",
    "        output, res_state = self._cell(inputs, state)\n",
    "        return tf.concat([output, res_state.attention], axis=-1), res_state\n",
    "\n",
    "    def zero_state(self, batch_size, dtype):\n",
    "        return self._cell.zero_state(batch_size, dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('root', reuse=tf.AUTO_REUSE):\n",
    "    char_embed_table = tf.get_variable('embedding', \n",
    "                            [len(hangul), 256], # number of symbols, embedding vector size\n",
    "                            dtype=tf.float32,\n",
    "                            initializer=tf.truncated_normal_initializer(stddev=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder:\n",
    "    # inputs: (batch, input_length)\n",
    "    def __init__(self, inputs, lengths, is_training):\n",
    "        \n",
    "        char_embedded_inputs = tf.nn.embedding_lookup(char_embed_table, inputs)\n",
    "        \n",
    "        # 3 convolution layers\n",
    "        x = char_embedded_inputs\n",
    "        with tf.variable_scope('prenet'):\n",
    "            layer_sizes = [256, 256, 256]\n",
    "            drop_rate = 0.1 if is_training else 0.0\n",
    "            for i, size in enumerate(layer_sizes):\n",
    "                conv_layer = tf.layers.Conv1D(filters=size, # number of output channels\n",
    "                                              kernel_size=5,\n",
    "                                              padding=\"same\",\n",
    "                                              activation=tf.nn.relu,\n",
    "                                              name=\"conv_{}\".format(i))\n",
    "                x = conv_layer.apply(x)\n",
    "                tf.layers.dropout(x, \n",
    "                                  rate=drop_rate, \n",
    "                                  name=\"dropout_{}\".format(i))\n",
    "        conv_result = x\n",
    "        \n",
    "        num_hidden = 128\n",
    "        lstm_fw_cell = tf.nn.rnn_cell.LSTMCell(num_hidden, forget_bias=1.0)\n",
    "        lstm_bw_cell = tf.nn.rnn_cell.LSTMCell(num_hidden, forget_bias=1.0)\n",
    "        outputs, rnn_states = tf.nn.bidirectional_dynamic_rnn(\n",
    "            cell_fw=lstm_fw_cell,\n",
    "            cell_bw=lstm_bw_cell,\n",
    "            inputs=conv_result,\n",
    "            sequence_length=lengths,\n",
    "            dtype=tf.float32)\n",
    "        output_concat = tf.concat(list(outputs), -1)\n",
    "        \n",
    "        self.output = output_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_scatter(indices, updates, shape):\n",
    "    updates = tf.reshape(updates, [-1, shape[2]])\n",
    "    indices = indices + tf.expand_dims(tf.range(0, shape[0]) * shape[1], 1)\n",
    "    indices = tf.reshape(indices, [-1, 1])\n",
    "\n",
    "    scatter = tf.scatter_nd(indices, updates, [shape[0]*shape[1], shape[2]])\n",
    "    scatter = tf.reshape(scatter, shape)\n",
    "    return scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder:\n",
    "    # encoder_outputs: (batch, input_length, 256)\n",
    "    # depend_targets: (batch, input_length)\n",
    "    def __init__(self, encoder_outputs, depend_targets, depend_idxs, morph_targets, out_lengths, is_training):\n",
    "        \n",
    "        cells = []\n",
    "        num_hidden = 256\n",
    "        keep_rate = 0.9 if is_training else 0.0\n",
    "        for layer_index in range(2):\n",
    "            lstm_cell = tf.nn.rnn_cell.LSTMCell(num_hidden, forget_bias=1.0)\n",
    "            cell = tf.contrib.rnn.DropoutWrapper(lstm_cell, input_keep_prob=keep_rate)\n",
    "            cells.append(cell)\n",
    "        prenet = tf.contrib.rnn.MultiRNNCell(cells)\n",
    "        \n",
    "        attention_size = 256\n",
    "        attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(\n",
    "            attention_size, \n",
    "            encoder_outputs,\n",
    "            normalize=True)\n",
    "        \n",
    "        attention_cell = ConcatOutputAndAttentionWrapper(\n",
    "            tf.contrib.seq2seq.AttentionWrapper(\n",
    "                prenet, \n",
    "                attention_mechanism,\n",
    "                output_attention=False))\n",
    "        \n",
    "        # lookup encoder outputs from dependency indices\n",
    "        self.depend_contexts = tf.batch_gather(encoder_outputs, depend_targets)\n",
    "        \n",
    "        shape = [tf.shape(encoder_outputs)[0], tf.shape(morph_targets)[1], encoder_outputs.shape[2]]\n",
    "        depend_contexts_sparse = batch_scatter(depend_idxs, self.depend_contexts, shape)\n",
    "        \n",
    "        # lookup char embeddings\n",
    "        morph_embedded = tf.nn.embedding_lookup(char_embed_table, morph_targets)\n",
    "        \n",
    "        decoder_inputs = tf.concat([morph_embedded, depend_contexts_sparse], axis=-1) # (batch, max_out_len, 512)\n",
    "        \n",
    "        # mask 15% of the lengths of the decoder inputs randomly\n",
    "        if is_training:\n",
    "        \n",
    "            batch_size = tf.shape(decoder_inputs)[0]\n",
    "            max_out_len = tf.shape(decoder_inputs)[1]\n",
    "            \n",
    "            mask_lengths = tf.to_int32(tf.to_float(out_lengths) * 0.15)\n",
    "            offset_bounds = out_lengths - mask_lengths - 1\n",
    "            offsets = tf.to_int32(tf.random.uniform([batch_size]) * tf.to_float(offset_bounds))\n",
    "            \n",
    "            rng = tf.range(max_out_len)\n",
    "            mask = tf.math.logical_and(\n",
    "                tf.expand_dims(rng, 0) >= tf.expand_dims(offsets, -1), \n",
    "                tf.expand_dims(rng, 0) < tf.expand_dims(offsets + mask_lengths, -1))\n",
    "            mask = tf.broadcast_to(tf.expand_dims(mask, -1), tf.shape(decoder_inputs))\n",
    "            \n",
    "            mask_symbol = tf.concat([char_embed_table[MASK_IDX], tf.zeros([num_hidden])], -1)\n",
    "            mask_symbol = tf.expand_dims(tf.expand_dims(mask_symbol, 0), 0)\n",
    "            mask_symbol = tf.broadcast_to(mask_symbol, tf.shape(decoder_inputs))\n",
    "            \n",
    "            decoder_inputs = tf.where(mask, mask_symbol, decoder_inputs)\n",
    "            \n",
    "        self.decoder_inputs = decoder_inputs\n",
    "        \n",
    "        output, rnn_states = tf.nn.dynamic_rnn(\n",
    "            cell=attention_cell,\n",
    "            inputs=decoder_inputs,\n",
    "            sequence_length=out_lengths,\n",
    "            dtype=tf.float32)\n",
    "        \n",
    "        self.char_output = tf.layers.dense(output[:, :, :num_hidden], len(hangul))\n",
    "        depend_output = output[:, :, num_hidden:]\n",
    "        \n",
    "        self.depend_output = tf.batch_gather(depend_output, depend_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, batch):\n",
    "        with tf.variable_scope('root', reuse=tf.AUTO_REUSE):\n",
    "            encoder = Encoder(batch['inputs'], batch['in_lengths'], True)\n",
    "            decoder = Decoder(encoder.output, batch['depends'], batch['depend_idxs'],\n",
    "                              batch['morphs'], batch['out_lengths'], True)\n",
    "\n",
    "            # dependency analyzer loss\n",
    "            depend_seq_loss = tf.norm(decoder.depend_contexts - decoder.depend_output, axis=-1)\n",
    "            \n",
    "            depend_mask = tf.math.equal(batch['depends'], 0)\n",
    "            depend_count = tf.to_float(tf.reduce_sum(batch['dep_lengths']))\n",
    "            \n",
    "            depend_masked_loss = tf.where(depend_mask, tf.zeros_like(depend_seq_loss), depend_seq_loss)\n",
    "            self.depend_loss = tf.reduce_sum(depend_masked_loss) / depend_count\n",
    "\n",
    "            # morpheme analyzer loss\n",
    "            char_seq_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                labels=batch['morphs'],\n",
    "                logits=decoder.char_output)\n",
    "\n",
    "            mask = tf.math.equal(batch['morphs'], PADDING_IDX)\n",
    "            total_length = tf.to_float(tf.reduce_sum(batch['out_lengths']))\n",
    "\n",
    "            char_masked_loss = tf.where(mask, tf.zeros_like(char_seq_loss), char_seq_loss)\n",
    "            self.char_loss = tf.reduce_sum(char_masked_loss) / total_length\n",
    "\n",
    "            # sum of the losses\n",
    "            self.total_loss = self.depend_loss + self.char_loss\n",
    "\n",
    "            # training-specific\n",
    "            self.global_step = tf.get_variable(\"global_step\", shape=[], trainable=False,\n",
    "                                      initializer=tf.zeros_initializer, dtype=tf.int32)\n",
    "\n",
    "            step = tf.cast(self.global_step + 1, dtype=tf.float32)\n",
    "\n",
    "            learning_rate = 1e-4 * tf.train.exponential_decay(1., step, 3000, 0.95)\n",
    "\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate, 0.9, 0.999)\n",
    "            gradients, variables = zip(*optimizer.compute_gradients(self.total_loss))\n",
    "            clipped_gradients, _ = tf.clip_by_global_norm(gradients, 1.0)\n",
    "\n",
    "            self.optimize = optimizer.apply_gradients(zip(clipped_gradients, variables), global_step=self.global_step)\n",
    "            \n",
    "            self.training_summary = tf.summary.merge([\n",
    "                tf.summary.scalar(\"total_loss\", self.total_loss),\n",
    "                tf.summary.scalar(\"char_loss\", self.char_loss),\n",
    "                tf.summary.scalar(\"depend_loss\", self.depend_loss)\n",
    "            ])\n",
    "            \n",
    "            self.validation_summary = tf.summary.merge([\n",
    "                tf.summary.scalar(\"validation_total_loss\", self.total_loss),\n",
    "                tf.summary.scalar(\"validation_char_loss\", self.char_loss),\n",
    "                tf.summary.scalar(\"validation_depend_loss\", self.depend_loss)\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "\n",
    "inputs_placeholder = tf.placeholder(name='inputs_placeholder', \n",
    "                                    shape=(None, max_in_length), \n",
    "                                    dtype=tf.int32)\n",
    "in_lengths_placeholder = tf.placeholder(name='in_lengths_placeholder',\n",
    "                                        shape=(None),\n",
    "                                        dtype=tf.int32)\n",
    "depend_idxs_placeholder = tf.placeholder(name='depend_idxs_placeholder',\n",
    "                                         shape=(None, max_dep_length),\n",
    "                                         dtype=tf.int32)\n",
    "depends_placeholder = tf.placeholder(name='depends_placeholder',\n",
    "                                     shape=(None, max_dep_length),\n",
    "                                     dtype=tf.int32)\n",
    "dep_lengths_placeholder = tf.placeholder(name='dep_lengths_placeholder',\n",
    "                                         shape=(None),\n",
    "                                         dtype=tf.int32)\n",
    "morphs_placeholder = tf.placeholder(name='morphs_placeholder',\n",
    "                                    shape=(None, max_out_length),\n",
    "                                    dtype=tf.int32)\n",
    "out_lengths_placeholder = tf.placeholder(name='out_lengths_placeholder',\n",
    "                                         shape=(None),\n",
    "                                         dtype=tf.int32)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices({\n",
    "    'inputs': inputs_placeholder,\n",
    "    'in_lengths': in_lengths_placeholder, \n",
    "    'depend_idxs': depend_idxs_placeholder,\n",
    "    'depends': depends_placeholder,\n",
    "    'dep_lengths': dep_lengths_placeholder,\n",
    "    'morphs': morphs_placeholder,\n",
    "    'out_lengths': out_lengths_placeholder\n",
    "})\n",
    "dataset = dataset.shuffle(buffer_size=10000)\n",
    "dataset = dataset.repeat()\n",
    "dataset = dataset.batch(batch_size)\n",
    "\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "\n",
    "model = Model(iterator.get_next())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run\n",
    "saver = tf.train.Saver()\n",
    "sess = tf.Session()\n",
    "\n",
    "train_writer = tf.summary.FileWriter(\"logs\", sess.graph)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(iterator.initializer, feed_dict={\n",
    "    inputs_placeholder: train['inputs'],\n",
    "    in_lengths_placeholder: train['in_lengths'],\n",
    "    depend_idxs_placeholder: train['depend_idxs'],\n",
    "    depends_placeholder: train['depends'],\n",
    "    dep_lengths_placeholder: train['dep_lengths'],\n",
    "    morphs_placeholder: train['morphs'],\n",
    "    out_lengths_placeholder: train['out_lengths']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40: 3.534392833709717\n",
      "Saved to models/charrnn-40\n",
      "50: 3.5312891006469727\n",
      "Saved to models/charrnn-50\n",
      "60: 3.4096686840057373\n",
      "Saved to models/charrnn-60\n",
      "70: 3.3832569122314453\n",
      "Saved to models/charrnn-70\n",
      "80: 3.2854151725769043\n",
      "Saved to models/charrnn-80\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    _, loss, step, log = sess.run((model.optimize, model.total_loss, model.global_step, model.training_summary))\n",
    "    \n",
    "    train_writer.add_summary(log, step)\n",
    "    \n",
    "    if step % 10 == 0:\n",
    "        print(\"{}: {}\".format(step, loss))\n",
    "        \n",
    "        save_path = saver.save(sess, \"models/charrnn\", step)\n",
    "        print('Saved to', save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
